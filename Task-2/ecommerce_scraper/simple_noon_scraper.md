### üß© **Purpose of the Script**

The goal of this script is to:

- Scrape product details (title, price, rating, link, image) for **gaming laptops** from **Noon Egypt**.
- Loop through multiple pages (1 to 5).
- Store the scraped data in a structured format (Pandas DataFrame).
- Save the results to a CSV file in a specific folder: `Task-2/ecommerce_scraper`.

---

### üîß **Step-by-Step Breakdown**

#### 1. **Import Required Libraries**

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
```

- `requests`: To send HTTP GET requests to fetch web page content.
- `BeautifulSoup` (from `bs4`): To parse and navigate the HTML content.
- `pandas`: To store and manage data in a tabular format (DataFrame) and export it.
- `time`: To add delays between requests (to be respectful to the server).

---

#### 2. **Initialize Empty Lists to Store Data**

```python
titles = []
prices = []
ratings = []
product_links = []
image_links = []
```

These lists will collect the scraped data for each product attribute:

- `titles`: Product names (e.g., "ASUS ROG Gaming Laptop")
- `prices`: Price displayed (e.g., "EGP 25,999")
- `ratings`: Customer rating (e.g., "4.7")
- `product_links`: Direct URL to the product page
- `image_links`: Link to the product image

---

#### 3. **Loop Through Multiple Pages**

```python
for page in range(1, 6):
    url = f"https://www.noon.com/egypt-en/eg-gaming-laptops/?page={page}"
```

- The script loops through **pages 1 to 5** of the "gaming laptops" category.
- Each URL is dynamically generated by inserting the `page` number.
- Example: `https://www.noon.com/egypt-en/eg-gaming-laptops/?page=1`

---

#### 4. **Define HTTP Headers**

```python
headers = {
    "accept": "...",
    "accept-language": "en,ar-AE;q=0.9,ar;q=0.8,en-US;q=0.7",
    "cache-control": "max-age=0",
    "cookie": "visitor_id=152f51b1-... (long string)",
    "priority": "u=0, i",
    "sec-ch-ua": "\"Not)A;Brand\";v=\"8\", \"Chromium\";v=\"138\", \"Google Chrome\";v=\"138\"",
    "sec-ch-ua-mobile": "?1",
    "sec-ch-ua-platform": "\"Android\"",
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "same-origin",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 ..."
}
```

##### üîç Why Are Headers Important?

Websites like Noon may block automated scripts (bots). To mimic a real browser:

- **User-Agent**: Pretends the request comes from a real mobile Chrome browser.
- **Accept-Language**: Shows preference for English and Arabic.
- **Cookie**: Includes session and tracking info ‚Äî helps bypass bot detection.
- **Sec-\* headers**: Security-related headers typical of modern browsers.
- Without these, the server might return:
  - Empty content
  - CAPTCHA
  - 403 Forbidden error

> ‚ö†Ô∏è Note: The `cookie` value here is **hardcoded and temporary** ‚Äî it may expire or stop working after a while.

---

#### 5. **Send Request and Fetch Page Content**

```python
response = requests.get(url, headers=headers)
print(f"Page {page} status:", response.status_code)
```

- Sends an HTTP GET request to the current page.
- Prints the HTTP status code:
  - `200` = success
  - `403` = forbidden (blocked)
  - `404` = not found

---

#### 6. **Parse HTML with BeautifulSoup**

```python
soup = BeautifulSoup(response.content, "html.parser")
```

- Converts raw HTML into a searchable structure.
- Allows us to find elements using CSS selectors.

---

#### 7. **Extract Product Containers**

```python
products = soup.select('a.ProductBoxLinkHandler_productBoxLink__FPhjp')
```

- Uses a **CSS selector** to find all product links (anchor tags with a specific class).
- Each `<a>` tag contains the full product box (title, price, image, etc.).

> üîç This class name (`ProductBoxLinkHandler_productBoxLink__FPhjp`) is dynamically generated (likely via React), so it might change in the future.

---

#### 8. **Check If Products Were Found**

```python
if not products:
    print(f"No products found on page {page}")
    break
```

- If no products are found, the loop stops early (e.g., if there are fewer than 5 pages).

---

#### 9. **Loop Through Each Product and Extract Data**

For **each product**, extract:

##### a. **Title**

```python
title_tag = product.select_one("h2.ProductDetailsSection_title__JorAV")
title = title_tag.text.strip() if title_tag else "N/A"
titles.append(title)
```

- Finds the `<h2>` with class `ProductDetailsSection_title__JorAV`.
- Gets its text and removes extra spaces.
- Appends to `titles` list.

##### b. **Price**

```python
price_tag = product.select_one("strong.Price_amount__2sXa7")
price = price_tag.text.strip() if price_tag else "N/A"
prices.append(price)
```

- Looks for `<strong>` tag with price class.
- Extracts price as string (e.g., "EGP 18,500").

##### c. **Rating**

```python
rating_tag = product.select_one("div.RatingPreviewStar_textCtr__sfsJG")
rating = rating_tag.text.strip() if rating_tag else "N/A"
ratings.append(rating)
```

- Gets the rating (e.g., "4.5") from the star rating text container.

##### d. **Product Link**

```python
href = product.get('href')
full_link = "https://www.noon.com" + href if href else "N/A"
product_links.append(full_link)
```

- Gets the relative `href` (e.g., `/product-name/12345?p=123`).
- Combines it with base domain to make a full URL.

> ‚ùó Fix: There was **extra space** in `"  https://www.noon.com  "` ‚Äî corrected to `"https://www.noon.com"`.

##### e. **Image Link**

```python
img_tag = product.select_one("img.ProductImageCarousel_productImage__jtsOn")
img_src = img_tag['src'] if img_tag else "N/A"
image_links.append(img_src)
```

- Finds the `<img>` tag inside the product card.
- Extracts the `src` attribute (image URL).

---

#### 10. **Add Delay Between Requests**

```python
time.sleep(1)
```

- Pauses for **1 second** between pages.
- Prevents overwhelming the server (ethical scraping practice).
- Helps avoid IP blocking.

---

#### 11. **Create a Pandas DataFrame**

```python
df = pd.DataFrame({
    "Product_name": titles,
    "Rating": ratings,
    "Price": prices,
    "Product_link": product_links,
    "Image_link": image_links
})
```

- Converts all lists into a structured table.
- Each row = one product.
- Columns = product attributes.

---

#### 12. **Display Summary of Scraped Data**

```python
print("\nDataFrame shape:", df.shape)
print("Number of items scraped:", len(df))
```

- `.shape` returns `(rows, columns)` ‚Äî e.g., `(50, 5)` means 50 products scraped.
- Confirms how much data was collected.

---

#### 13. **Save Data to CSV File**

```python
import os

output_dir = "Task-2/ecommerce_scraper"
os.makedirs(output_dir, exist_ok=True)

output_file = os.path.join(output_dir, "noon_gaming_laptops_v1.csv")

df.to_csv(output_file, index=False, encoding='utf-8')

print(f"\n‚úÖ Data saved to: {output_file}")
```

- `os.makedirs(..., exist_ok=True)`: Creates the folder `Task-2/ecommerce_scraper` if it doesn't exist.
- `to_csv()`:
  - Saves the DataFrame as a CSV file.
  - `index=False`: Doesn‚Äôt save row numbers.
  - `encoding='utf-8'`: Supports Arabic characters, symbols (like EGP), emojis.
- Final message confirms successful save.

> ‚úÖ Output example:  
> `Task-2/ecommerce_scraper/noon_gaming_laptops_v1.csv`

---

### üìå Summary of What This Script Does

| Step | Action                                                |
| ---- | ----------------------------------------------------- |
| 1    | Requests 5 pages of gaming laptops from Noon Egypt    |
| 2    | Mimics a real user with realistic headers and cookies |
| 3    | Parses HTML to extract product details                |
| 4    | Stores data in lists                                  |
| 5    | Builds a clean DataFrame                              |
| 6    | Saves the data to a CSV file in a designated folder   |

---

### ‚ö†Ô∏è Limitations & Risks

| Issue                   | Explanation                                                                                                    |
| ----------------------- | -------------------------------------------------------------------------------------------------------------- |
| **Hardcoded Cookies**   | Session may expire; scraping might fail after a few hours.                                                     |
| **Dynamic Class Names** | If Noon updates its frontend, class names like `ProductBoxLinkHandler_...` could change, breaking the scraper. |
| **Anti-Bot Protection** | Advanced sites use tools like Cloudflare, which may block this script over time.                               |
| **Rate Limiting**       | Even with `time.sleep(1)`, too many requests can get your IP banned.                                           |

---

### ‚úÖ Suggestions for Improvement

1. **Use Retry Logic**  
   Add `try-except` blocks and retry failed requests.

2. **Rotate User Agents**  
   Use different `User-Agent` strings to reduce detection risk.

3. **Check Response Content**  
   Verify that the HTML contains expected content before parsing.

4. **Log Errors**  
   Record failed pages or missing data for debugging.

5. **Use Selenium (Optional)**  
   For JavaScript-heavy sites, consider `Selenium` or `Playwright`.

---

### ‚úÖ Final Output Example (CSV)

| Product_name              | Rating | Price      | Product_link             | Image_link           |
| ------------------------- | ------ | ---------- | ------------------------ | -------------------- |
| ASUS TUF Gaming Laptop... | 4.7    | EGP 22,999 | https://www.noon.com/... | https://...image.jpg |
| Lenovo Legion 5 Pro...    | 4.6    | EGP 31,500 | https://www.noon.com/... | https://...img2.jpg  |

---

### ‚úÖ Conclusion

This is a **well-structured, functional web scraper** that:

- Efficiently collects product data from Noon Egypt.
- Handles pagination.
- Saves results in a usable format.
- Follows basic ethical scraping practices (delays, headers).
