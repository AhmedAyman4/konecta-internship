{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypDnZRsRS6Rm"
   },
   "source": [
    "# Konecta Internship Task 6 (RAG System for PDF Manuals)\n",
    "\n",
    "**Name:** Ahmed Ayman Ahmed Alhofy  \n",
    "**Track:** Artificial Intelligence & Machine Learning  \n",
    "**Repository Link:** [https://github.com/AhmedAyman4/konecta-internship/tree/main/Task-6](https://github.com/AhmedAyman4/konecta-internship/tree/main/Task-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "291a5dc7",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community langchain-google-genai pypdf langchain_huggingface chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ5qNooXVCh8"
   },
   "source": [
    "### What is Retrieval-Augmented Generation (RAG)?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** is a method that improves large language models by adding external knowledge. Instead of guessing or using built-in knowledge, RAG first **retrieves** relevant information from a document (like a manual or PDF), then uses that info to **generate** accurate answers.\n",
    "\n",
    "This helps avoid fake or incorrect answers (hallucinations) and is perfect for tasks like answering questions about technical guides, company docs, or any specific dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### How RAG Works – Simple Steps\n",
    "\n",
    "1. **Load Documents**  \n",
    "   Read files (like PDFs) using tools like `PyPDFLoader`.\n",
    "\n",
    "2. **Split Text**  \n",
    "   Break text into small chunks so the model can process them easily.\n",
    "\n",
    "3. **Create Embeddings**  \n",
    "   Convert each chunk into numbers (vectors) that represent its meaning.\n",
    "\n",
    "4. **Store in Vector Database**  \n",
    "   Save these vectors in a search-friendly database like Chroma or FAISS.\n",
    "\n",
    "5. **Retrieve on Query**  \n",
    "   When you ask a question, find the most relevant text chunks.\n",
    "\n",
    "6. **Add Context to Prompt**  \n",
    "   Give the LLM both your question and the retrieved text.\n",
    "\n",
    "7. **Generate Answer**  \n",
    "   The model answers based only on the provided context — no guessing.\n",
    "\n",
    "8. **(Optional) Support Chat History**  \n",
    "   Handle follow-up questions by remembering past messages.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Use RAG?\n",
    "- ✅ Reduces hallucinations  \n",
    "- ✅ Shows sources for answers  \n",
    "- ✅ Works with your own documents  \n",
    "- ✅ No need to train or fine-tune the model  \n",
    "\n",
    "In this task, RAG is used to answer questions about device manuals — giving accurate, cited responses from real product guides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHKk6K4VPvc5"
   },
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "This cell sets up the environment and imports necessary libraries for document loading, text processing, embedding, vector storage, and conversational retrieval using LangChain and Google's Generative AI. It also configures warnings to be suppressed for cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gbkExduijNFN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.colab import userdata\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpPP_JBLP4bx"
   },
   "source": [
    "### Load PDF Documents\n",
    "\n",
    "This cell loads multiple PDF manuals from specified file paths using `PyPDFLoader` and aggregates them into a single document list. Each loaded document retains its source metadata for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiKb91wwsJKZ",
    "outputId": "0122db3f-81d2-49e7-a9f0-d6e230ee05f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x292363 for key /Im30\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x292c33 for key /Im220\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x292e4f for key /Im13\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x292f6c for key /Im275\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x2d683b for key /Im250\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x2d684a for key /Im251\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x2d6868 for key /Im250\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x2d6877 for key /Im251\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x2d6976 for key /Im252\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x2d6985 for key /Im251\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x2d6994 for key /Im253\n",
      "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x2d6ac1 for key /Im250\n"
     ]
    }
   ],
   "source": [
    "# Create a list of paths to your PDF manuals\n",
    "pdf_paths = [\n",
    "    \"/content/Data/legion_5.pdf\",\n",
    "    \"/content/Data/loq_series.pdf\",\n",
    "    \"/content/Data/zbook_14u_g6.pdf\",\n",
    "    \"/content/Data/samsung_odyssey_g9_series.pdf\"\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for path in pdf_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    # The loader automatically adds document source metadata\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkC5X9Z1QGm_"
   },
   "source": [
    "### Prepare Model and Split Documents\n",
    "\n",
    "Initializes the Google Generative AI model (`gemini-1.5-flash`) for fast inference and sets up a text splitter to break documents into manageable chunks. Metadata such as document name and page number are preserved for context tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EW15A-Kapvwz"
   },
   "outputs": [],
   "source": [
    "google_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None, # Max output tokens\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    convert_system_message_to_human=True,\n",
    "    google_api_key=userdata.get('GOOGLE_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwcc2S_5fC5s"
   },
   "source": [
    "### Split Documents into Chunks\n",
    "\n",
    "Splits loaded PDFs into smaller chunks (1000 characters each, with 100-character overlap) for better retrieval. Metadata like document name and page number is preserved for source tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L1emd6C0t8bM"
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Attach extra metadata manually (doc name, page number)\n",
    "for c in chunks:\n",
    "    c.metadata[\"doc_name\"] = c.metadata.get(\"source\", \"Unknown\")\n",
    "    c.metadata[\"page_number\"] = c.metadata.get(\"page\", \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jk6ly_sUGsl",
    "outputId": "a385426f-9ff8-4bda-ece3-0bd148bcf1bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PDFlib+PDI 9.0.7p3 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2021-06-15T10:52:52+08:00', 'configfile': 'E:\\\\program files\\\\ptc\\\\arbortext pe\\\\custom\\\\app\\\\standard.appcf', 'stylesheet': 'E:\\\\program files\\\\ptc\\\\arbortext pe\\\\custom\\\\doctypes\\\\ditabase\\\\8.5x11_book_hel.style', 'ptcarbortextbuild': 'R70M060-65', 'title': 'User Guide', 'epsprocessor': 'PStill version 1.84.42', 'printengine': 'PTC Arbortext Advanced Print Publisher 11.1.4334/W Library-x64', 'moddate': '2021-06-15T10:52:52+08:00', 'source': '/content/Data/legion_5.pdf', 'total_pages': 34, 'page': 14, 'page_label': '15', 'doc_name': '/content/Data/legion_5.pdf', 'page_number': 14}, page_content='5 15ACH6A and Lenovo Legion 5 15ACH6A \\n– Input: 100 V ac–240 V ac, 50 Hz–60 Hz\\n– Output: 20 V dc, 11.5 A/20 V dc, 15 A\\n– Power: 230 W/300 W\\nBattery pack\\n• 15-inch models \\n– Capacity: 60 Wh/80 Wh\\n– Number of cells: 4\\n• 17-inch models \\n– Capacity: 80 Wh\\n– Number of cells: 4\\nNote: The battery capacity is the typical or average capacity as measured in a specific test \\nenvironment. Capacities measured in other environments may differ but are no lower than the \\nrated capacity (see product label).\\nMicroprocessor To view the microprocessor information of your computer, type system information in the \\nWindows search box and then press Enter.\\nMemory\\n• Type: Double data rate 4 (DDR4) small outline dual in-line memory module (SODIMM)\\n• Number of slots: 2\\nChapter 1 . Meet your computer 9')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zj8SJ3_XuYyz",
    "outputId": "001edd1b-6b6f-4a7e-850b-5f965ae73b10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ullkJYswQsXo"
   },
   "source": [
    "### Embedding Model and Vector Store Selection\n",
    "\n",
    "We use the **`all-MiniLM-L6-v2`** sentence transformer model from Hugging Face, which efficiently maps text to a 384-dimensional dense vector space, making it ideal for semantic search and retrieval. It offers a strong balance between performance and speed for our use case.\n",
    "\n",
    "**Chroma** is chosen as the vector store over **FAISS** for several reasons:\n",
    "- **Ease of Use**: Chroma is designed to be developer-friendly, with straightforward APIs for adding, querying, and persisting data.\n",
    "- **Native Persistence**: Chroma supports built-in persistence with minimal configuration (via `persist_directory`), whereas FAISS requires manual serialization (e.g., saving/loading indexes and embedding matrices separately).\n",
    "- **LangChain Integration**: Chroma has seamless, first-party integration with LangChain, simplifying retrieval chain setup.\n",
    "- **Metadata Support**: Chroma natively supports metadata filtering, which is useful for source tracking (e.g., filtering by document or page).\n",
    "\n",
    "While **FAISS** (by Facebook) is highly optimized for performance and scalability in large-scale similarity search, it is more complex to manage and lacks built-in persistence and metadata handling out of the box. For this project, where simplicity, rapid prototyping, and local persistence are prioritized over massive scale, **Chroma** is the more suitable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "I7EyXAIGvePX"
   },
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b3ce327",
    "outputId": "e9853184-9fe4-4dae-e1bb-c8605228e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.chroma.Chroma object at 0x785c7aa9ef30>\n"
     ]
    }
   ],
   "source": [
    "# Create a Chroma vector store\n",
    "# Specify a persist_directory to save the collection locally\n",
    "vectorstore = Chroma.from_documents(chunks, hf, persist_directory=\"./chroma_manuals_db\")\n",
    "\n",
    "# To load it later:\n",
    "# vectorstore = Chroma(persist_directory=\"./chroma_manuals_db\", embedding_function=hf)\n",
    "\n",
    "# Display the vector store\n",
    "print(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2G8g6FNMAaJ"
   },
   "source": [
    "### Setting Up the Retrieval and QA Chains\n",
    "\n",
    "This cell configures the retrieval and question-answering pipelines:\n",
    "- A **retriever** fetches the top 3 relevant document chunks from the vector store.\n",
    "- A **contextualization chain** converts follow-up questions into standalone queries using chat history.\n",
    "- A **question-answering chain** generates concise, context-bound answers using the retrieved documents, ensuring responses are grounded in the provided manuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KvHO_dAS0xil"
   },
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# System prompt to turn a follow-up question into a standalone question\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might refer to chat history, formulate a standalone question which can be used \\\n",
    "to retrieve relevant documents. Do not answer the question, just reformulate it if needed \\\n",
    "and otherwise return it as is.\"\"\"\n",
    "\n",
    "# Prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain that makes the question standalone\n",
    "contextualize_q_chain = contextualize_q_prompt | google_llm | StrOutputParser()\n",
    "\n",
    "# System prompt for answering based on retrieved context\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "You must strictly use only the retrieved context below to answer.\n",
    "If the answer is not contained in the provided context, say: \"I don't know based on the manuals.\"\n",
    "Do not use outside knowledge.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "# Prompt template for answering questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain that answers the question based on the context\n",
    "question_answer_chain = create_stuff_documents_chain(google_llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvVQepWMMIwW"
   },
   "source": [
    "### Building the Conversational RAG Pipeline\n",
    "\n",
    "This cell assembles a **Retrieval-Augmented Generation (RAG)** chain with source tracking and chat history support:\n",
    "- Combines question contextualization, retrieval, and answer generation.\n",
    "- Enhances responses with source metadata (document name, page number).\n",
    "- Uses `RunnableWithMessageHistory` to maintain conversational memory across turns, enabling context-aware interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WNZjlRKfMEnL"
   },
   "outputs": [],
   "source": [
    "# Combine contextualization and question-answering chains\n",
    "def with_sources(inputs):\n",
    "    \"\"\"Ensure output contains both the answer and the sources.\"\"\"\n",
    "    answer = inputs.get(\"answer\", \"\")\n",
    "    docs = inputs.get(\"context\", [])\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"doc_name\": d.metadata.get(\"doc_name\", \"Unknown\"),\n",
    "                \"page_number\": d.metadata.get(\"page_number\", \"N/A\"),\n",
    "            }\n",
    "            for d in docs if hasattr(d, \"metadata\")\n",
    "        ],\n",
    "    }\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=contextualize_q_chain | retriever)\n",
    "    | {\n",
    "        \"answer\": question_answer_chain,\n",
    "        \"context\": lambda x: x[\"context\"],  # keep the retrieved docs\n",
    "    }\n",
    "    | with_sources\n",
    ")\n",
    "\n",
    "# In-memory storage for chat history\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Conversational RAG chain with chat history\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SO9JnbXPSE7n"
   },
   "source": [
    "### Running Example Q&A Interactions\n",
    "\n",
    "This cell demonstrates the conversational RAG system by asking a series of questions, including follow-ups and cross-document queries. The chatbot responds using only the retrieved context, cites sources, and maintains conversation history. A 5-second delay between calls ensures compliance with API rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjeNHPkc6Ms0",
    "outputId": "55a558a3-0b8c-4757-9f1e-e51df2f3706d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Q&A interactions...\n",
      "\n",
      "User: What is the maximum supported RAM on the Legion 5?\n",
      "Chatbot: I don't know based on the manuals. The provided context specifies the memory type and the number of slots, but it does not state the maximum supported RAM capacity.\n",
      "  ↳ Source: /content/Data/legion_5.pdf (page 14)\n",
      "  ↳ Source: /content/Data/legion_5.pdf (page 14)\n",
      "  ↳ Source: /content/Data/legion_5.pdf (page 14)\n",
      "\n",
      "User: How many memory slots does it have?\n",
      "Chatbot: I don't know based on the manuals. The provided context does not contain information about the number of memory slots.\n",
      "  ↳ Source: /content/Data/legion_5.pdf (page 0)\n",
      "  ↳ Source: /content/Data/legion_5.pdf (page 0)\n",
      "  ↳ Source: /content/Data/legion_5.pdf (page 0)\n",
      "\n",
      "User: What is the maximum refresh rate of the Samsung Odyssey G9?\n",
      "Chatbot: For the C49G9*T* model, which is the Samsung Odyssey G9, the maximum refresh rate is 240 Hz. This maximum refresh rate is supported with DisplayPort1 and DisplayPort2. The function is disabled when the HDMI port is in use.\n",
      "  ↳ Source: /content/Data/samsung_odyssey_g9_series.pdf (page 18)\n",
      "  ↳ Source: /content/Data/samsung_odyssey_g9_series.pdf (page 18)\n",
      "  ↳ Source: /content/Data/samsung_odyssey_g9_series.pdf (page 19)\n",
      "\n",
      "User: How to upgrade the RAM on the ZBook 14u G6?\n",
      "Chatbot: I don't know based on the manuals. The provided context does not contain information on how to upgrade the RAM on the ZBook 14u G6.\n",
      "  ↳ Source: /content/Data/zbook_14u_g6.pdf (page 51)\n",
      "  ↳ Source: /content/Data/zbook_14u_g6.pdf (page 51)\n",
      "  ↳ Source: /content/Data/zbook_14u_g6.pdf (page 51)\n",
      "\n",
      "User: What is the capital of France?\n",
      "Chatbot: I don't know based on the manuals.\n",
      "  ↳ Source: /content/Data/samsung_odyssey_g9_series.pdf (page 43)\n",
      "  ↳ Source: /content/Data/samsung_odyssey_g9_series.pdf (page 43)\n",
      "  ↳ Source: /content/Data/samsung_odyssey_g9_series.pdf (page 14)\n",
      "\n",
      "Q&A interactions finished.\n"
     ]
    }
   ],
   "source": [
    "# Example Q&A interactions\n",
    "questions = [\n",
    "    \"What is the maximum supported RAM on the Legion 5?\", # Direct factual\n",
    "    \"How many memory slots does it have?\", # Follow-up\n",
    "    \"What is the maximum refresh rate of the Samsung Odyssey G9?\", # Different manual\n",
    "    \"How to upgrade the RAM on the ZBook 14u G6?\", # Another manual\n",
    "    \"What is the capital of France?\" # Out-of-scope\n",
    "]\n",
    "\n",
    "session_id = \"example_session\" # Use a consistent session ID for follow-up questions\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting Q&A interactions...\\n\")\n",
    "for question in questions:\n",
    "    print(f\"User: {question}\")\n",
    "    result = conversational_rag_chain.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(f\"Chatbot: {result['answer']}\")\n",
    "    if result[\"sources\"]:\n",
    "        for src in result[\"sources\"]:\n",
    "            print(f\"  ↳ Source: {src['doc_name']} (page {src['page_number']})\")\n",
    "    print()\n",
    "\n",
    "    time.sleep(5)  # wait 5 sec between calls to stay under 10 RPM\n",
    "\n",
    "print(\"Q&A interactions finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jWRTOm_ffIA"
   },
   "source": [
    "### Q&A Interaction Results\n",
    "\n",
    "The RAG system responded to each query using only the information retrieved from the manuals:\n",
    "- Correctly identified when answers were **not available** in the documents.\n",
    "- Provided accurate details for supported questions (e.g., Samsung Odyssey G9 refresh rate).\n",
    "- Cited sources with document name and page number for transparency.\n",
    "- Handled out-of-scope questions (like general knowledge) safely by not hallucinating.\n",
    "\n",
    "This demonstrates the system’s ability to **retrieve, reason, and respond** with context-awareness and source tracking."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
