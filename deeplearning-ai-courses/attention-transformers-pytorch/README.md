### Attention in Transformers: Concepts and Code in PyTorch

**Course Link:**  
https://learn.deeplearning.ai/courses/attention-in-transformers-concepts-and-code-in-pytorch

---

#### Agenda

1. Introduction
2. The Main Ideas Behind Transformers and Attention
3. The Matrix Math for Calculating Self-Attention
4. Coding Self-Attention in PyTorch
5. Self-Attention vs Masked Self-Attention
6. The Matrix Math for Calculating Masked Self-Attention
7. Coding Masked Self-Attention in PyTorch
8. Encoder-Decoder Attention
9. Multi-Head Attention
10. Coding Encoder-Decoder Attention and Multi-Head Attention in PyTorch
11. Conclusion
